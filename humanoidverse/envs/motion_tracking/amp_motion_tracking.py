import torch
import numpy as np
from humanoidverse.envs.motion_tracking.motion_tracking import LeggedRobotMotionTracking
from isaac_utils.rotations import (
    my_quat_rotate,
    calc_heading_quat_inv,
    quat_mul,
    quat_conjugate,
    quat_rotate_inverse,
)
from humanoidverse.utils.torch_utils import quat_rotate, normalize
from loguru import logger

def quat_to_tan_norm(q):
    """
    Convert quaternion to tangent-normal form (6D representation)
    Based on the paper "On the Continuity of Rotation Representations in Neural Networks"
    """
    # Normalize quaternion
    q = normalize(q)
    
    # è·å–æ—‹è½¬çŸ©é˜µçš„å‰ä¸¤åˆ—ä½œä¸º6Dè¡¨ç¤º
    # q = [x, y, z, w] format
    w, x, y, z = q[..., 3], q[..., 0], q[..., 1], q[..., 2]
    
    # è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µçš„å‰ä¸¤åˆ—
    # First column of rotation matrix
    col1_x = 1 - 2 * (y*y + z*z)
    col1_y = 2 * (x*y + w*z)
    col1_z = 2 * (x*z - w*y)
    
    # Second column of rotation matrix  
    col2_x = 2 * (x*y - w*z)
    col2_y = 1 - 2 * (x*x + z*z)
    col2_z = 2 * (y*z + w*x)
    
    # ç»„åˆæˆ6Dè¡¨ç¤º
    col1 = torch.stack([col1_x, col1_y, col1_z], dim=-1)
    col2 = torch.stack([col2_x, col2_y, col2_z], dim=-1)
    
    return torch.cat([col1, col2], dim=-1)

class AMPExpertDataCache:
    """AMPä¸“å®¶æ•°æ®é¢„åŠ è½½å’Œç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, device, cache_size=10000):
        self.device = device
        self.cache_size = cache_size
        self.data_cache = None
        self.cache_filled = False
        self.cache_index = 0
        
    def initialize_cache(self, amp_obs_dim):
        """åˆå§‹åŒ–ç¼“å­˜"""
        self.data_cache = torch.zeros(self.cache_size, amp_obs_dim, device=self.device)
        logger.info(f"åˆå§‹åŒ–AMPæ•°æ®ç¼“å­˜: {self.cache_size} x {amp_obs_dim}")
    
    def add_data(self, data):
        """æ·»åŠ æ•°æ®åˆ°ç¼“å­˜"""
        if self.data_cache is None:
            return False
            
        batch_size = data.shape[0]
        if self.cache_index + batch_size <= self.cache_size:
            self.data_cache[self.cache_index:self.cache_index + batch_size] = data
            self.cache_index += batch_size
        else:
            # ç¯å½¢ç¼“å†²
            remaining = self.cache_size - self.cache_index
            self.data_cache[self.cache_index:] = data[:remaining]
            self.data_cache[:batch_size - remaining] = data[remaining:]
            self.cache_index = batch_size - remaining
            self.cache_filled = True
            
        return True
    
    def sample(self, batch_size):
        """ä»ç¼“å­˜ä¸­é‡‡æ ·"""
        if self.data_cache is None:
            return None
            
        if not self.cache_filled and self.cache_index < batch_size:
            # ç¼“å­˜æ•°æ®ä¸è¶³
            return self.data_cache[:self.cache_index] if self.cache_index > 0 else None
            
        # éšæœºé‡‡æ ·
        if self.cache_filled:
            indices = torch.randperm(self.cache_size, device=self.device)[:batch_size]
        else:
            indices = torch.randperm(self.cache_index, device=self.device)[:batch_size]
            
        return self.data_cache[indices]
    
    def get_cache_size(self):
        """è·å–ç¼“å­˜ä¸­çš„æ•°æ®é‡"""
        if not self.cache_filled:
            return self.cache_index
        return self.cache_size
    
    def is_cache_ready(self):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å‡†å¤‡å¥½"""
        return self.cache_filled or self.cache_index >= self.cache_size // 10  # è‡³å°‘10%æ•°æ®

class AMPMotionTracking(LeggedRobotMotionTracking):
    def __init__(self, config, device):
        # åœ¨è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ä¹‹å‰ï¼Œå…ˆä¿®å¤AMPè§‚æµ‹ç»´åº¦é…ç½®
        self._fix_amp_obs_config(config)
        
        # æ ‡è®°åˆå§‹åŒ–çŠ¶æ€
        self.init_done = False
        super().__init__(config, device)
        
        # è®¾ç½®å…³é”®èº«ä½“ç‚¹ç´¢å¼•ï¼ˆç”¨äºAMPè§‚æµ‹ï¼‰
        self._setup_key_body_ids()
        
        # åˆå§‹åŒ–AMPæ•°æ®ç¼“å­˜ç³»ç»Ÿ
        self._init_amp_data_cache()
        
        # æ£€æŸ¥è¯„ä¼°æ¨¡å¼çŠ¶æ€
        logger.info(f"åˆå§‹åŒ–å®Œæˆåçš„è¯„ä¼°æ¨¡å¼çŠ¶æ€: {getattr(self, 'is_evaluating', False)}")
        
        # å»¶è¿Ÿåˆå§‹åŒ–AMPæ•°æ®ï¼Œç­‰å¾… set_is_evaluating è°ƒç”¨
        self.amp_data_initialized = False
        self._init_amp_data()
        self.init_done = True

    def _fix_amp_obs_config(self, config):
        """ä¿®å¤AMPè§‚æµ‹ç»´åº¦é…ç½® - ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼"""
        try:
            # ç¡®ä¿åŸºç¡€é…ç½®å­˜åœ¨
            if not hasattr(config.robot, "algo_obs_dim_dict"):
                config.robot.algo_obs_dim_dict = {}
            
            # å®‰å…¨åœ°è·å–DOFè§‚æµ‹å¤§å°
            dof_obs_size = getattr(config.robot, 'dof_obs_size', 30)  # é»˜è®¤30
            
            # é»˜è®¤ä½¿ç”¨2ä¸ªå…³é”®èº«ä½“ç‚¹ï¼ˆè„šéƒ¨ï¼‰
            num_key_bodies = 2
            
            # è®¡ç®—æ ‡å‡†AMPè§‚æµ‹ç»´åº¦
            # root_h(1) + root_rot(6) + root_vel(3) + root_ang_vel(3) + dof_obs + dof_vel + key_body_pos
            expected_amp_dim = 1 + 6 + 3 + 3 + dof_obs_size + dof_obs_size + (3 * num_key_bodies)
            
            # æ›´æ–°é…ç½®ä¸­çš„amp_obsç»´åº¦
            config.robot.algo_obs_dim_dict["amp_obs"] = expected_amp_dim
            
            logger.info(f"è®¾ç½®AMPè§‚æµ‹ç»´åº¦ä¸º: {expected_amp_dim}")
            
        except Exception as e:
            logger.warning(f"é…ç½®AMPè§‚æµ‹ç»´åº¦æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
            if not hasattr(config.robot, "algo_obs_dim_dict"):
                config.robot.algo_obs_dim_dict = {}
            config.robot.algo_obs_dim_dict["amp_obs"] = 79  # é»˜è®¤å€¼

    def _setup_key_body_ids(self):
        """è®¾ç½®å…³é”®èº«ä½“ç‚¹ç´¢å¼• - ä½¿ç”¨å®‰å…¨çš„æ–¹æ³•"""
        try:
            # ä¼˜å…ˆä½¿ç”¨è„šéƒ¨ç´¢å¼•
            if hasattr(self, 'feet_indices') and len(self.feet_indices) > 0:
                self._key_body_ids = self.feet_indices
                logger.info(f"ä½¿ç”¨è„šéƒ¨ç´¢å¼•ä½œä¸ºå…³é”®èº«ä½“ç‚¹: {self._key_body_ids}")
            else:
                # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
                self._key_body_ids = torch.tensor([6, 12], device=self.device)
                logger.warning(f"ä½¿ç”¨é»˜è®¤å…³é”®èº«ä½“ç‚¹ç´¢å¼•: {self._key_body_ids}")
                
        except Exception as e:
            logger.error(f"è®¾ç½®å…³é”®èº«ä½“ç‚¹æ—¶å‡ºé”™: {e}")
            # ä½¿ç”¨æœ€å®‰å…¨çš„é»˜è®¤å€¼
            self._key_body_ids = torch.tensor([6, 12], device=self.device)

    def _init_amp_data_cache(self):
        """åˆå§‹åŒ–AMPæ•°æ®ç¼“å­˜ç³»ç»Ÿ"""
        try:
            amp_obs_dim = self.config.robot.algo_obs_dim_dict["amp_obs"]
            cache_size = getattr(self.config, 'amp_cache_size', 20000)
            
            # åˆ›å»ºä¸“å®¶æ•°æ®ç¼“å­˜
            self.expert_data_cache = AMPExpertDataCache(self.device, cache_size)
            self.expert_data_cache.initialize_cache(amp_obs_dim)
            
            logger.info(f"AMPæ•°æ®ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜å¤§å°: {cache_size}")
            
        except Exception as e:
            logger.error(f"AMPæ•°æ®ç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")

    def _init_amp_data(self):
        """åˆå§‹åŒ–AMPç›¸å…³æ•°æ® - ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–"""
        try:
            # è·å–AMPè§‚æµ‹ç»´åº¦
            amp_obs_dim = self.config.robot.algo_obs_dim_dict["amp_obs"]
            
            # åˆ›å»ºAMPè§‚æµ‹ç¼“å†²åŒº
            self.amp_obs_buf = torch.zeros(
                self.num_envs, 
                amp_obs_dim, 
                device=self.device
            )
            
            logger.info(f"åˆå§‹åŒ–AMPè§‚æµ‹ç¼“å†²åŒºï¼Œå½¢çŠ¶: {self.amp_obs_buf.shape}")
            
            # å¼‚æ­¥é¢„åŠ è½½expertæ•°æ®
            self._preload_expert_data_async()
            
            # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–
            self.amp_data_initialized = True
            
            logger.info("AMPæ•°æ®åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"AMPæ•°æ®åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä½¿ç”¨æœ€å°çš„fallback
            amp_obs_dim = 79
            self.amp_obs_buf = torch.zeros(self.num_envs, amp_obs_dim, device=self.device)
            self.amp_data_initialized = False

    def _preload_expert_data_async(self):
        """å¼‚æ­¥é¢„åŠ è½½expertæ•°æ®åˆ°ç¼“å­˜"""
        try:
            is_eval = getattr(self, 'is_evaluating', False)
            target_samples = 5000 if is_eval else 20000
            batch_size = 500
            
            loaded_count = 0
            max_attempts = target_samples // batch_size + 10
            
            for attempt in range(max_attempts):
                if loaded_count >= target_samples:
                    break
                    
                try:
                    # æ‰¹é‡ç”Ÿæˆexpertæ•°æ®
                    expert_batch = self._generate_expert_batch(batch_size)
                    if expert_batch is not None and expert_batch.shape[0] > 0:
                        success = self.expert_data_cache.add_data(expert_batch)
                        if success:
                            loaded_count += expert_batch.shape[0]
                            
                            if attempt % 10 == 0:
                                logger.info(f"é¢„åŠ è½½expertæ•°æ®è¿›åº¦: {loaded_count}/{target_samples}")
                        else:
                            logger.warning(f"ç¼“å­˜æ•°æ®å¤±è´¥ (attempt {attempt})")
                    else:
                        logger.warning(f"ç”Ÿæˆexpertæ•°æ®å¤±è´¥ (attempt {attempt})")
                        
                except Exception as e:
                    logger.warning(f"é¢„åŠ è½½expertæ•°æ®æ—¶å‡ºé”™ (attempt {attempt}): {e}")
                    continue
            
            final_cache_size = self.expert_data_cache.get_cache_size()
            logger.info(f"Expertæ•°æ®é¢„åŠ è½½å®Œæˆ: {final_cache_size}/{target_samples}")
            
        except Exception as e:
            logger.error(f"é¢„åŠ è½½expertæ•°æ®å¤±è´¥: {e}")

    def _generate_expert_batch(self, batch_size):
        """æ‰¹é‡ç”Ÿæˆexpertæ•°æ®"""
        try:
            if not hasattr(self, '_motion_lib') or self._motion_lib is None:
                return None
                
            num_motions = self._motion_lib._num_unique_motions
            if num_motions == 0:
                return None
                
            expert_states = []
            
            # éšæœºé€‰æ‹©motionå’Œæ—¶é—´ç‚¹
            motion_ids = torch.randint(0, num_motions, (batch_size,), device=self.device)
            
            for i in range(batch_size):
                try:
                    motion_id = motion_ids[i].item()
                    motion_length = self._motion_lib.get_motion_length([motion_id]).item()
                    
                    # éšæœºé‡‡æ ·æ—¶é—´ç‚¹
                    time = torch.rand(1, device=self.device) * max(0.1, motion_length - 0.1)
                    
                    motion_state = self._motion_lib.get_motion_state(
                        torch.tensor([motion_id], device=self.device),
                        time,
                        offset=torch.zeros(1, 3, device=self.device)
                    )
                    
                    amp_obs = self._build_amp_obs_from_state(motion_state)
                    if amp_obs is not None:
                        expert_states.append(amp_obs)
                        
                except Exception as e:
                    logger.debug(f"ç”Ÿæˆå•ä¸ªexpertæ ·æœ¬å¤±è´¥: {e}")
                    continue
            
            if len(expert_states) > 0:
                return torch.stack(expert_states)
            else:
                return None
                
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”Ÿæˆexpertæ•°æ®å¤±è´¥: {e}")
            return None

    def _build_amp_obs_from_state(self, motion_state):
        """ä»motionçŠ¶æ€æ„å»ºæ ‡å‡†AMPè§‚æµ‹"""
        try:
            # æ£€æŸ¥å¿…è¦çš„çŠ¶æ€æ˜¯å¦å­˜åœ¨
            required_keys = ["root_pos", "root_rot", "root_vel", "root_ang_vel", "dof_pos", "dof_vel"]
            for key in required_keys:
                if key not in motion_state:
                    logger.error(f"MotionçŠ¶æ€ç¼ºå°‘ {key}")
                    return None
            
            root_pos = motion_state["root_pos"][0]      # [3]
            root_rot = motion_state["root_rot"][0]      # [4] 
            root_vel = motion_state["root_vel"][0]      # [3]
            root_ang_vel = motion_state["root_ang_vel"][0]  # [3]
            dof_pos = motion_state["dof_pos"][0]        # [30]
            dof_vel = motion_state["dof_vel"][0]        # [30]
            
            # å®‰å…¨åœ°è·å–å…³é”®èº«ä½“ç‚¹
            if "rg_pos_t" in motion_state:
                all_body_pos = motion_state["rg_pos_t"][0]  # [num_bodies, 3]
                num_bodies = all_body_pos.shape[0]
                
                # ç¡®ä¿æ‰€æœ‰å…³é”®èº«ä½“ç‚¹ç´¢å¼•éƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…
                valid_key_body_ids = []
                for idx in self._key_body_ids:
                    if idx < num_bodies:
                        valid_key_body_ids.append(idx)
                    else:
                        logger.warning(f"å…³é”®èº«ä½“ç‚¹ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡")
                
                if len(valid_key_body_ids) > 0:
                    # ä½¿ç”¨æœ‰æ•ˆçš„ç´¢å¼•
                    valid_indices = torch.tensor(valid_key_body_ids, device=self.device)
                    key_body_pos = all_body_pos[valid_indices]
                    
                    # å¦‚æœæœ‰æ•ˆç´¢å¼•æ•°é‡ä¸è¶³ï¼Œç”¨é›¶å‘é‡è¡¥å……
                    if len(valid_key_body_ids) < len(self._key_body_ids):
                        missing_count = len(self._key_body_ids) - len(valid_key_body_ids)
                        zeros = torch.zeros(missing_count, 3, device=self.device)
                        key_body_pos = torch.cat([key_body_pos, zeros], dim=0)
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç´¢å¼•ï¼Œä½¿ç”¨é›¶å‘é‡
                    key_body_pos = torch.zeros(len(self._key_body_ids), 3, device=self.device)
                    logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å…³é”®èº«ä½“ç‚¹ç´¢å¼•ï¼Œä½¿ç”¨é›¶å‘é‡")
            else:
                # å¦‚æœæ²¡æœ‰rg_pos_tæ•°æ®ï¼Œä½¿ç”¨é›¶å‘é‡
                key_body_pos = torch.zeros(len(self._key_body_ids), 3, device=self.device)
                logger.warning("æ²¡æœ‰rg_pos_tæ•°æ®ï¼Œä½¿ç”¨é›¶å‘é‡")
            
            # æ„å»ºæ ‡å‡†AMPè§‚æµ‹
            amp_obs = self._build_standard_amp_obs(
                root_pos.unsqueeze(0),      # [1, 3]
                root_rot.unsqueeze(0),      # [1, 4]
                root_vel.unsqueeze(0),      # [1, 3] 
                root_ang_vel.unsqueeze(0),  # [1, 3]
                dof_pos.unsqueeze(0),       # [1, 30]
                dof_vel.unsqueeze(0),       # [1, 30]
                key_body_pos.unsqueeze(0)   # [1, num_key_bodies, 3]
            )
            
            return amp_obs.squeeze(0)
            
        except Exception as e:
            logger.error(f"æ„å»ºAMPè§‚æµ‹æ—¶å‡ºé”™: {e}")
            return None

    def _build_standard_amp_obs(self, root_pos, root_rot, root_vel, root_ang_vel, 
                               dof_pos, dof_vel, key_body_pos):
        """æ ‡å‡†AMPè§‚æµ‹æ„å»ºï¼Œå‚è€ƒhumanoidå®ç°"""
        try:
            # æ ¹èŠ‚ç‚¹é«˜åº¦
            root_h = root_pos[:, 2:3]
            
            # è®¡ç®—heading rotationï¼ˆå»é™¤pitchå’Œrollï¼‰
            heading_rot = calc_heading_quat_inv(root_rot, w_last=True)
            
            # å±€éƒ¨åæ ‡ç³»ä¸‹çš„æ ¹èŠ‚ç‚¹æ—‹è½¬ï¼ˆè½¬æ¢ä¸º6Dè¡¨ç¤ºï¼‰
            local_root_rot = quat_mul(heading_rot, root_rot, w_last=True)
            root_rot_obs = quat_to_tan_norm(local_root_rot)
            
            # å±€éƒ¨åæ ‡ç³»ä¸‹çš„æ ¹èŠ‚ç‚¹é€Ÿåº¦
            local_root_vel = my_quat_rotate(heading_rot, root_vel)
            local_root_ang_vel = my_quat_rotate(heading_rot, root_ang_vel)
            
            # å±€éƒ¨åæ ‡ç³»ä¸‹çš„å…³é”®èº«ä½“ç‚¹ä½ç½®
            root_pos_expand = root_pos.unsqueeze(-2)
            local_key_body_pos = key_body_pos - root_pos_expand
            
            # è½¬æ¢å…³é”®èº«ä½“ç‚¹åˆ°å±€éƒ¨åæ ‡ç³»
            heading_rot_expand = heading_rot.unsqueeze(-2)
            heading_rot_expand = heading_rot_expand.repeat((1, local_key_body_pos.shape[1], 1))
            flat_local_key_pos = my_quat_rotate(
                heading_rot_expand.view(-1, 4), 
                local_key_body_pos.view(-1, 3)
            ).view(local_key_body_pos.shape[0], -1)
            
            # DOFè§‚æµ‹å¤„ç†
            dof_obs = self._process_dof_obs(dof_pos)
            
            # ç»„åˆæœ€ç»ˆè§‚æµ‹ - ä¸¥æ ¼æŒ‰ç…§æ ‡å‡†æ ¼å¼
            obs = torch.cat([
                root_h,                 # [B, 1] æ ¹èŠ‚ç‚¹é«˜åº¦
                root_rot_obs,          # [B, 6] å±€éƒ¨æ ¹èŠ‚ç‚¹æ—‹è½¬ï¼ˆ6Dï¼‰
                local_root_vel,        # [B, 3] å±€éƒ¨æ ¹èŠ‚ç‚¹çº¿é€Ÿåº¦  
                local_root_ang_vel,    # [B, 3] å±€éƒ¨æ ¹èŠ‚ç‚¹è§’é€Ÿåº¦
                dof_obs,              # [B, 30] å¤„ç†åçš„å…³èŠ‚ä½ç½®
                dof_vel,              # [B, 30] å…³èŠ‚é€Ÿåº¦
                flat_local_key_pos,   # [B, 6] å±€éƒ¨å…³é”®èº«ä½“ç‚¹ä½ç½®ï¼ˆ2ä¸ªè„š * 3ç»´ï¼‰
            ], dim=-1)
            
            return obs
            
        except Exception as e:
            logger.error(f"æ„å»ºæ ‡å‡†AMPè§‚æµ‹æ—¶å‡ºé”™: {e}")
            # è¿”å›é›¶è§‚æµ‹ä½œä¸ºfallback
            amp_obs_dim = self.config.robot.algo_obs_dim_dict["amp_obs"]
            return torch.zeros(root_pos.shape[0], amp_obs_dim, device=self.device)

    def _process_dof_obs(self, dof_pos):
        """å¤„ç†DOFè§‚æµ‹ - å‚è€ƒæ ‡å‡†å®ç°"""
        try:
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è¿”å›ç›¸å¯¹äºé»˜è®¤ä½ç½®çš„åç§»
            if hasattr(self, 'default_dof_pos') and self.default_dof_pos is not None:
                return dof_pos - self.default_dof_pos.squeeze(0)
            else:
                return dof_pos
        except:
            return dof_pos

    def _compute_amp_observations(self):
        """è®¡ç®—å½“å‰çŠ¶æ€çš„æ ‡å‡†AMPè§‚æµ‹"""
        try:
            # è·å–å…³é”®èº«ä½“ç‚¹ä½ç½®
            if hasattr(self, '_rigid_body_pos_extend'):
                all_body_pos = self._rigid_body_pos_extend
            else:
                all_body_pos = self.simulator._rigid_body_pos
            
            # å®‰å…¨åœ°é€‰æ‹©å…³é”®èº«ä½“ç‚¹
            num_bodies = all_body_pos.shape[1]
            valid_key_body_ids = []
            
            for idx in self._key_body_ids:
                if idx < num_bodies:
                    valid_key_body_ids.append(idx)
            
            if len(valid_key_body_ids) > 0:
                valid_indices = torch.tensor(valid_key_body_ids, device=self.device)
                key_body_pos = all_body_pos[:, valid_indices, :]
                
                # å¦‚æœæœ‰æ•ˆç´¢å¼•æ•°é‡ä¸è¶³ï¼Œç”¨é›¶å‘é‡è¡¥å……
                if len(valid_key_body_ids) < len(self._key_body_ids):
                    missing_count = len(self._key_body_ids) - len(valid_key_body_ids)
                    zeros = torch.zeros(self.num_envs, missing_count, 3, device=self.device)
                    key_body_pos = torch.cat([key_body_pos, zeros], dim=1)
            else:
                key_body_pos = torch.zeros(self.num_envs, len(self._key_body_ids), 3, device=self.device)
            
            # æ„å»ºæ ‡å‡†AMPè§‚æµ‹ï¼Œç¡®ä¿ä¸expertæ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´
            amp_obs = self._build_standard_amp_obs(
                self.simulator.robot_root_states[:, :3],      # root_pos [N, 3]
                self.simulator.robot_root_states[:, 3:7],     # root_rot [N, 4]
                self.simulator.robot_root_states[:, 7:10],    # root_vel [N, 3]
                self.simulator.robot_root_states[:, 10:13],   # root_ang_vel [N, 3]
                self.simulator.dof_pos,                       # dof_pos [N, 30]
                self.simulator.dof_vel,                       # dof_vel [N, 30]
                key_body_pos                                  # key_body_pos [N, num_key_bodies, 3]
            )
            
            self.amp_obs_buf[:] = amp_obs
            return amp_obs
            
        except Exception as e:
            logger.error(f"è®¡ç®—AMPè§‚æµ‹æ—¶å‡ºé”™: {e}")
            return self.amp_obs_buf

    def get_expert_amp_observations(self, num_samples=None):
        """è·å–ä¸“å®¶AMPè§‚æµ‹ - ä½¿ç”¨ç¼“å­˜ç³»ç»Ÿä¼˜åŒ–"""
        if num_samples is None:
            num_samples = self.num_envs
        
        try:
            # ä»ç¼“å­˜ä¸­è·å–æ•°æ®
            if hasattr(self, 'expert_data_cache') and self.expert_data_cache.is_cache_ready():
                expert_data = self.expert_data_cache.sample(num_samples)
                
                if expert_data is not None and expert_data.shape[0] > 0:
                    # å¦‚æœç¼“å­˜æ•°æ®ä¸è¶³ï¼Œè¡¥å……ç”Ÿæˆ
                    if expert_data.shape[0] < num_samples:
                        additional_needed = num_samples - expert_data.shape[0]
                        additional_data = self._generate_expert_batch(additional_needed)
                        
                        if additional_data is not None:
                            # å°†æ–°ç”Ÿæˆçš„æ•°æ®æ·»åŠ åˆ°ç¼“å­˜
                            self.expert_data_cache.add_data(additional_data)
                            # ç»„åˆæ•°æ®
                            expert_data = torch.cat([expert_data, additional_data[:additional_needed]], dim=0)
                    
                    # ç¡®ä¿æ•°æ®è´¨é‡
                    if torch.isnan(expert_data).any() or torch.isinf(expert_data).any():
                        logger.error("ç¼“å­˜çš„expertæ•°æ®åŒ…å«NaNæˆ–Infï¼")
                        expert_data = self._generate_fallback_data(num_samples)
                    
                    logger.debug(f"ä»ç¼“å­˜è·å–expertè§‚æµ‹: shape={expert_data.shape}, "
                                f"mean={expert_data.mean().item():.4f}, "
                                f"std={expert_data.std().item():.4f}")
                    
                    return expert_data
            
            # å¦‚æœç¼“å­˜ä¸å¯ç”¨ï¼Œç”Ÿæˆæ–°æ•°æ®
            logger.warning("Expertæ•°æ®ç¼“å­˜ä¸å¯ç”¨ï¼Œç”Ÿæˆæ–°æ•°æ®")
            expert_data = self._generate_expert_batch(num_samples)
            
            if expert_data is None or expert_data.shape[0] == 0:
                logger.error("æ— æ³•ç”Ÿæˆexpertæ•°æ®ï¼Œä½¿ç”¨fallback")
                return self._generate_fallback_data(num_samples)
            
            return expert_data
            
        except Exception as e:
            logger.error(f"è·å–expertè§‚æµ‹æ—¶å‡ºé”™: {e}")
            return self._generate_fallback_data(num_samples)

    def _generate_fallback_data(self, num_samples):
        """ç”Ÿæˆfallbackæ•°æ®"""
        amp_obs_dim = self.config.robot.algo_obs_dim_dict["amp_obs"]
        return torch.zeros(num_samples, amp_obs_dim, device=self.device)

    def _pre_compute_observations_callback(self):
        """åœ¨è®¡ç®—è§‚æµ‹ä¹‹å‰çš„å›è°ƒï¼Œç¡®ä¿AMPè§‚æµ‹è¢«æ›´æ–°"""
        super()._pre_compute_observations_callback()
        self._compute_amp_observations()

    def _get_obs_amp_obs(self):
        """è·å–AMPè§‚æµ‹ï¼Œç”¨äºobsç³»ç»Ÿ"""
        if not hasattr(self, 'amp_obs_buf') or self.amp_obs_buf is None:
            self._compute_amp_observations()
        return self.amp_obs_buf

    def _compute_observations(self):
        """é‡å†™è§‚æµ‹è®¡ç®—ï¼Œç¡®ä¿AMPè§‚æµ‹è¢«åŒ…å«"""
        self._compute_amp_observations()
        super()._compute_observations()

    def _init_buffers(self):
        """åˆå§‹åŒ–ç¼“å†²åŒº"""
        super()._init_buffers()
        self.terminate_when_motion_far_threshold = self.config.termination_curriculum.terminate_when_motion_far_initial_threshold

        # éªŒè¯AMPè§‚æµ‹ç»´åº¦
        expected_amp_dim = self.config.robot.algo_obs_dim_dict["amp_obs"]
        logger.info(f"AMPè§‚æµ‹ç»´åº¦: {expected_amp_dim}")

    def set_is_evaluating(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ - AMPç‰¹æ®Šå¤„ç†"""
        logger.info("ğŸ”„ AMPMotionTracking åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼")
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        super().set_is_evaluating()
        
        # é‡æ–°é…ç½®AMPæ•°æ®ä¸ºè¯„ä¼°æ¨¡å¼
        if hasattr(self, 'expert_data_cache'):
            logger.info("é‡æ–°é…ç½®AMPæ•°æ®ç¼“å­˜ä¸ºè¯„ä¼°æ¨¡å¼")
            try:
                # æ¸…ç©ºç¼“å­˜å¹¶é‡æ–°é¢„åŠ è½½
                self.expert_data_cache.cache_index = 0
                self.expert_data_cache.cache_filled = False
                self._preload_expert_data_async()
                logger.info("âœ… AMPæ•°æ®ç¼“å­˜å·²åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼")
            except Exception as e:
                logger.error(f"âŒ AMPè¯„ä¼°æ¨¡å¼é‡æ–°åˆå§‹åŒ–å¤±è´¥: {e}")

    def _post_physics_step(self):
        """é‡å†™åå¤„ç†æ­¥éª¤"""
        super()._post_physics_step()
        
        # å®šæœŸè¡¥å……expertæ•°æ®ç¼“å­˜
        if hasattr(self, 'expert_data_cache') and self.common_step_counter % 1000 == 0:
            try:
                if not self.expert_data_cache.is_cache_ready():
                    # å¼‚æ­¥è¡¥å……ç¼“å­˜
                    additional_data = self._generate_expert_batch(500)
                    if additional_data is not None:
                        self.expert_data_cache.add_data(additional_data)
                        logger.debug(f"è¡¥å……expertç¼“å­˜æ•°æ®: {additional_data.shape[0]} æ ·æœ¬")
            except Exception as e:
                logger.debug(f"è¡¥å……expertç¼“å­˜å¤±è´¥: {e}")

    def _reset_tasks_callback(self, env_ids):
        if len(env_ids) == 0:
            return
        super()._reset_tasks_callback(env_ids)
        self._resample_motion_times(env_ids) # need to resample before reset root states
        if self.config.termination.terminate_when_motion_far and self.config.termination_curriculum.terminate_when_motion_far_curriculum:
            self._update_terminate_when_motion_far_curriculum()
    
    def _update_terminate_when_motion_far_curriculum(self):
        assert self.config.termination.terminate_when_motion_far and self.config.termination_curriculum.terminate_when_motion_far_curriculum
        if self.average_episode_length < self.config.termination_curriculum.terminate_when_motion_far_curriculum_level_down_threshold:
            self.terminate_when_motion_far_threshold *= (1 + self.config.termination_curriculum.terminate_when_motion_far_curriculum_degree)
        elif self.average_episode_length > self.config.termination_curriculum.terminate_when_motion_far_curriculum_level_up_threshold:
            self.terminate_when_motion_far_threshold *= (1 - self.config.termination_curriculum.terminate_when_motion_far_curriculum_degree)
        self.terminate_when_motion_far_threshold = np.clip(self.terminate_when_motion_far_threshold, 
                                                         self.config.termination_curriculum.terminate_when_motion_far_threshold_min, 
                                                         self.config.termination_curriculum.terminate_when_motion_far_threshold_max)
